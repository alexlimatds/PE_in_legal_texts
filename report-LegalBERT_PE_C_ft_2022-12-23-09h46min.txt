RESULTS REPORT
Model: LegalBERT_PE_C
Encoder: nlpaueb/legal-bert-base-uncased
Evaluation: test set (many random seeds)
Train scheme: fine-tuning
Features combination: concatenation
Batch size: 16
Dropout rate: 0.1
Learning rate: 1e-05
mvPE step: 1
Max sentence length: 512
Adam Epsilon: 1e-08
LR warmup: True
Use MLP: False
Use Normalization layer: False
Weight decay: 0.001
Train time: 11h28m20s
GPU name: Quadro RTX 6000
GPU memory: 22.17

Averages:
Epoch Train loss    std    Test loss    std    Precision  P std   Recall  R std     F1    F1 std
   1   0.386001   0.001223  0.358396  0.025809   0.6918   0.0854  0.5632  0.1195  0.6037  0.0469
   2   0.339890   0.005885  0.372723  0.010423   0.6773   0.0639  0.5464  0.0989  0.5936  0.0351
   3   0.334417   0.003788  0.389651  0.022739   0.6616   0.0714  0.5482  0.1070  0.5861  0.0374
   4   0.316827   0.004006  0.381866  0.004203   0.6805   0.0053  0.5209  0.0121  0.5900  0.0067

*** Detailed report ***

Confusion matrices
------------------
Facts: 0 
Other: 1 
=> Iteration 0:
Epoch 1:
[[ 863  832]
 [ 324 6027]]
Epoch 2:
[[1184  511]
 [ 889 5462]]
Epoch 3:
[[1203  492]
 [1051 5300]]
Epoch 4:
[[ 908  787]
 [ 443 5908]]
=> Iteration 1:
Epoch 1:
[[1046  649]
 [ 450 5901]]
Epoch 2:
[[ 972  723]
 [ 470 5881]]
Epoch 3:
[[ 925  770]
 [ 432 5919]]
Epoch 4:
[[ 904  791]
 [ 420 5931]]
=> Iteration 2:
Epoch 1:
[[1259  436]
 [1050 5301]]
Epoch 2:
[[ 991  704]
 [ 521 5830]]
Epoch 3:
[[ 781  914]
 [ 295 6056]]
Epoch 4:
[[ 864  831]
 [ 407 5944]]
=> Iteration 3:
Epoch 1:
[[ 960  735]
 [ 453 5898]]
Epoch 2:
[[ 739  956]
 [ 248 6103]]
Epoch 3:
[[ 695 1000]
 [ 262 6089]]
Epoch 4:
[[ 857  838]
 [ 388 5963]]
=> Iteration 4:
Epoch 1:
[[ 645 1050]
 [ 153 6198]]
Epoch 2:
[[ 745  950]
 [ 266 6085]]
Epoch 3:
[[1042  653]
 [ 585 5766]]
Epoch 4:
[[ 882  813]
 [ 416 5935]]

Scores
------
Epoch: 1
             Train loss  Test loss  Precision    Recall        F1
Iteration 0    0.386597   0.340390   0.727043  0.509145  0.598890
Iteration 1    0.385738   0.338249   0.699198  0.617109  0.655594
Iteration 2    0.387492   0.399908   0.545258  0.742773  0.628871
Iteration 3    0.386348   0.335841   0.679406  0.566372  0.617761
Iteration 4    0.383832   0.377592   0.808271  0.380531  0.517449

Epoch: 2
             Train loss  Test loss  Precision    Recall        F1
Iteration 0    0.335989   0.382105   0.571153  0.698525  0.628450
Iteration 1    0.333953   0.362220   0.674064  0.573451  0.619700
Iteration 2    0.341824   0.358452   0.655423  0.584661  0.618023
Iteration 3    0.350465   0.383790   0.748734  0.435988  0.551081
Iteration 4    0.337217   0.377050   0.736894  0.439528  0.550628

Epoch: 3
             Train loss  Test loss  Precision    Recall        F1
Iteration 0    0.332924   0.422227   0.533718  0.709735  0.609268
Iteration 1    0.331104   0.365484   0.681651  0.545723  0.606160
Iteration 2    0.333612   0.379674   0.725836  0.460767  0.563695
Iteration 3    0.341812   0.410921   0.726228  0.410029  0.524133
Iteration 4    0.332630   0.369950   0.640443  0.614749  0.627333

Epoch: 4
             Train loss  Test loss  Precision    Recall        F1
Iteration 0    0.314738   0.386735   0.672095  0.535693  0.596192
Iteration 1    0.314295   0.384564   0.682779  0.533333  0.598874
Iteration 2    0.315374   0.384341   0.679780  0.509735  0.582603
Iteration 3    0.324810   0.377483   0.688353  0.505605  0.582993
Iteration 4    0.314918   0.376207   0.679507  0.520354  0.589375

